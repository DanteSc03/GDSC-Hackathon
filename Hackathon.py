# -*- coding: utf-8 -*-
"""Hackathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DsgpHNOrnavO3Rvd3Do2_BqajVKpcvS3
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/MyDrive/Colab\ Notebooks/

"""#First we must import all necessary libraries"""

# Commented out IPython magic to ensure Python compatibility.
import csv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

"""###Next we need to import our data"""

train = pd.read_csv("Train_tesla.csv")
test = pd.read_csv("Test_tesla.csv")

"""###After importing our data we must clean our data utilizing different methods to convert categorical data into numerical data"""

spending_score_mapping = {'High': 3, 'Average': 2, 'Low': 1}

train['Spending_Score'] = train['Spending_Score'].map(spending_score_mapping)

train['Family_Size'] = (train['Family_Size'] - train['Family_Size'].min()) / (train['Family_Size'].max() - train['Family_Size'].min())

cat_columns = ['Gender', 'Graduated', 'Profession', 'Ever_Married', 'Var_1']

for col in cat_columns:
  train[col] = pd.Categorical(train[col])
  train[col] = train[col].cat.codes
train.to_csv('train_tesla_cat.csv', index = False)

train

"""###Fill the NaN values with the mode or mean depending on if it is categorical or a numeric column"""

numerical_columns = ['Family_Size','Ever_Married','Graduated','Work_Experience']

categorical_columns = ['Profession', 'Var_1']

for col in numerical_columns:
  train[col].fillna(train[col].mean(), inplace = True)

for col in categorical_columns:
  train[col].fillna(train[col].mode(), inplace = True)

scaler = MinMaxScaler()

numerical_columns2 = ['Age', 'Work_Experience', 'Spending_Score']
train[numerical_columns2] = scaler.fit_transform(train[numerical_columns2])


train

"""###Now we must split the training data into x and y"""

y = train['Segmentation']
X = train.drop(['Segmentation','ID'], axis=1)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state= 42)

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_val.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_val.shape}")

"""###After splitting the data we utilize our GradientBoostingClassifier to check how accurate our model is"""

model = GradientBoostingClassifier(n_estimators=96, learning_rate=0.1, max_depth=3, random_state=42,
                                   warm_start = True)

model.fit(X_train, y_train)

y_predicted = model.predict(X_val)

accuracy = accuracy_score(y_val, y_predicted) *100
print(f"Accuracy: {accuracy:.2f} %")
print(classification_report(y_val, y_predicted))

"""###Now we can put our results into a more comprehensive graph to see where we have made our mistakes"""

cm = confusion_matrix(y_val, y_predicted)
cm

plt.figure(figsize = (10,7))
sns.heatmap(cm, annot = True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

"""#Test Data

###After we have completed our training we must apply the same preprocesses to our new data for segmentation
"""

spending_score_mapping = {'High': 3, 'Average': 2, 'Low': 1}

test['Spending_Score'] = test['Spending_Score'].map(spending_score_mapping)

test['Family_Size'] = (test['Family_Size'] - train['Family_Size'].min()) / (train['Family_Size'].max() - train['Family_Size'].min())

cat_columns = ['Gender', 'Graduated', 'Profession', 'Ever_Married', 'Var_1']

for col in cat_columns:
  test[col] = pd.Categorical(test[col])
  test[col] = test[col].cat.codes

numerical_columns = ['Family_Size','Ever_Married','Work_Experience', 'Age']

categorical_columns = ['Profession', 'Var_1', 'Gender','Graduated', 'Spending_Score' ]

for col in numerical_columns:
  test[col].fillna(train[col].mean(), inplace = True)

for col in categorical_columns:
  test[col].fillna(train[col].mode(), inplace = True)

scaler = MinMaxScaler()

numerical_columns2 = ['Age', 'Work_Experience']
test[numerical_columns2] = scaler.fit_transform(test[numerical_columns2])

test = test.round(0)

model = GradientBoostingClassifier(n_estimators=96, learning_rate=0.1, max_depth=3, random_state=42,
                                   warm_start = True)
model.fit(X_train, y_train)

final_hackathon = model.predict(test.drop(['ID'], axis=1, errors='ignore'))

final_hackathon = pd.DataFrame({
    "ID": test["ID"],
    "Segmentation": final_hackathon
})

final_hackathon.to_csv('final_hackathon2.csv', index=False)